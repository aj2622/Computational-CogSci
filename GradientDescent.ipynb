{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GradientDescent.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klYzQZtoqqpA",
        "colab_type": "text"
      },
      "source": [
        "This notebook implements the gradient descent algorithm for a simple artificial neuron. The neuron will learn to compute logical OR. The neuron model and logical OR, for inputs $x_0$ and $x_1$ and target output $y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIyjopHaocYi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "from __future__ import print_function\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8ZfieYmrPZT",
        "colab_type": "text"
      },
      "source": [
        "We create `torch.tensor` objects for representing the data matrix `D` with targets `Y`. Each row of `D` is a different data point."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Iu7HZU_omt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D = np.zeros((4,2),dtype=float)\n",
        "D[0,:] = [0.,0.]\n",
        "D[1,:] = [0.,1.]\n",
        "D[2,:] = [1.,0.]\n",
        "D[3,:] = [1.,1.]\n",
        "D = torch.tensor(D,dtype=torch.float)\n",
        "Y = torch.tensor([0.,1.,1.,1.])\n",
        "N = D.shape[0] # number of input patterns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK5Jm-WBreBf",
        "colab_type": "text"
      },
      "source": [
        "The artificial neuron operates as follows. Given an input vector $x$, the net input ($\\textbf{net}$) to the neuron is computed as follows\n",
        "\n",
        "$$ \\textbf{net} = \\sum_i x_i w_i + b,$$\n",
        "\n",
        "for weights $w_i$ and bias $b$. The activation function $g(\\textbf{net})$ is the logistic function\n",
        "\n",
        "$$ g(\\textbf{net}) = \\frac{1}{1+e^{-\\textbf{net}}},$$\n",
        "\n",
        "which is used to compute the predicted output $\\hat{y} = g(\\textbf{net})$. Finally, the loss (squared error) for a particular pattern $x$ is defined as \n",
        "\n",
        "$$ E(w,b) = (\\hat{y}-y)^2,$$\n",
        "\n",
        "where the target output is $y$. Here we manually compute the gradients of the loss $E$ with respect to the neuron parameters:\n",
        "\n",
        "$$\\frac{\\partial E(w,b)}{\\partial w}, \\frac{\\partial E(w,b)}{\\partial b}.$$\n",
        "\n",
        "By manually, we mean to program the gradient computation directly, using the formulas discussed in class. This is in contrast to using PyTorch's `autograd` (Automatric differentiation) that computes the gradient automatically, as discussed in class, lab, and in the PyTorch tutorial (e.g., `loss.backward()`). First, let's write the activation function and the loss in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOuJBUnGqK9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g_logistic(net):\n",
        "    return 1. / (1.+torch.exp(-net))\n",
        "\n",
        "def loss(yhat,y):\n",
        "    return (yhat-y)**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr74hj89r5mH",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll also write two functions for examining the internal operations of the neuron, and the gradients of its parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFN0j0Z-qLro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_forward(x,yhat,y):\n",
        "    # Examine network's prediction for input x\n",
        "    print(' Input: ',end='')\n",
        "    print(x.numpy())\n",
        "    print(' Output: ' + str(round(yhat.item(),3)))\n",
        "    print(' Target: ' + str(y.item()))\n",
        "\n",
        "def print_grad(grad_w,grad_b):\n",
        "    # Examine gradients\n",
        "    print('  d_loss / d_w = ',end='')\n",
        "    print(grad_w)\n",
        "    print('  d_loss / d_b = ',end='')\n",
        "    print(grad_b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh8bo6fKr9ZM",
        "colab_type": "text"
      },
      "source": [
        "Now let's begin the implementation of stochastic gradient descent. We'll initialize our parameters $w$ and $b$ randomly, and proceed through a series of epochs of training. Each epoch involves visiting the four training patterns in random order, and updating the parameters after each presentation of an input pattern.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "um7pN6eYqPLN",
        "colab_type": "code",
        "outputId": "73155ed8-9e6e-475a-dfe8-2749d8373e01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize parameters\n",
        "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
        "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
        "w = torch.rand(2,requires_grad=True) # [size 2] tensor\n",
        "b = torch.rand(1,requires_grad=True) # [size 1] tensor\n",
        "\n",
        "alpha = 0.05 # learning rate\n",
        "nepochs = 2000 # number of epochs\n",
        "\n",
        "track_error = []\n",
        "verbose = True\n",
        "for e in range(nepochs): # for each epoch\n",
        "    error_epoch = 0. # sum loss across the epoch\n",
        "    perm = np.random.permutation(N)\n",
        "    for p in perm: # visit data points in random order\n",
        "        x = D[p,:] # input pattern\n",
        "        \n",
        "        # compute output of neuron\n",
        "        net = torch.dot(x,w)+b\n",
        "        yhat = g_logistic(net)\n",
        "        \n",
        "        # compute loss\n",
        "        y = Y[p]\n",
        "        myloss = loss(yhat,y)\n",
        "        error_epoch += myloss.item()\n",
        "        \n",
        "        # print output if this is the last epoch\n",
        "        if (e == nepochs-1):\n",
        "            print(\"Final result:\")\n",
        "            print_forward(x,yhat,y)\n",
        "            print(\"\")\n",
        "\n",
        "        # Compute the gradient manually\n",
        "        if verbose:\n",
        "            print('Compute the gradient manually')\n",
        "            print_forward(x,yhat,y)\n",
        "        with torch.no_grad():\n",
        "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
        "            #  two lines of the form\n",
        "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
        "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
        "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
        "            #   otherwise PyTorch will try to track the \"gradient\" of the gradient computation, which we don't want.\n",
        "            w_grad = 2*(yhat - y)*yhat*(1 - yhat)*x\n",
        "            b_grad = 2*(yhat - y)*yhat*(1 - yhat)\n",
        "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
        "\n",
        "        # Compute the gradient with PyTorch and compre with manual values\n",
        "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
        "        myloss.backward()\n",
        "        if verbose:\n",
        "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
        "            print(\"\")\n",
        "        w.grad.zero_() # clear PyTorch's gradient\n",
        "        b.grad.zero_()\n",
        "        \n",
        "        # Parameter update with gradient descent\n",
        "        with torch.no_grad():\n",
        "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
        "            #  two lines of the form:\n",
        "            #    w -=   ....\n",
        "            #    b -=   ....\n",
        "            w -= w_grad\n",
        "            b -= b_grad \n",
        "            \n",
        "    if verbose==True: verbose=False\n",
        "    track_error.append(error_epoch)\n",
        "    if e % 50 == 0:\n",
        "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
        "    \n",
        "# track output of gradient descent\n",
        "plt.figure()\n",
        "plt.clf()\n",
        "plt.plot(track_error)\n",
        "plt.title('stochastic gradient descent (logistic activation)')\n",
        "plt.ylabel('error for epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute the gradient manually\n",
            " Input: [1. 0.]\n",
            " Output: 0.779\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-0.07623821 -0.        ]\n",
            "  d_loss / d_b = [-0.07623821]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [-0.07623821 -0.        ]\n",
            "  d_loss / d_b = [-0.07623821]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [1. 1.]\n",
            " Output: 0.835\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-0.04542459 -0.04542459]\n",
            "  d_loss / d_b = [-0.04542459]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [-0.04542459 -0.04542459]\n",
            "  d_loss / d_b = [-0.04542459]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [0. 1.]\n",
            " Output: 0.723\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-0.         -0.11125127]\n",
            "  d_loss / d_b = [-0.11125127]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [ 0.         -0.11125129]\n",
            "  d_loss / d_b = [-0.11125129]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [0. 0.]\n",
            " Output: 0.692\n",
            " Target: 0.0\n",
            "  d_loss / d_w = [0. 0.]\n",
            "  d_loss / d_b = [0.29492965]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [0. 0.]\n",
            "  d_loss / d_b = [0.29492962]\n",
            "\n",
            "epoch 0; error=0.633\n",
            "epoch 50; error=0.07\n",
            "epoch 100; error=0.032\n",
            "epoch 150; error=0.02\n",
            "epoch 200; error=0.015\n",
            "epoch 250; error=0.011\n",
            "epoch 300; error=0.009\n",
            "epoch 350; error=0.008\n",
            "epoch 400; error=0.007\n",
            "epoch 450; error=0.006\n",
            "epoch 500; error=0.005\n",
            "epoch 550; error=0.005\n",
            "epoch 600; error=0.004\n",
            "epoch 650; error=0.004\n",
            "epoch 700; error=0.004\n",
            "epoch 750; error=0.003\n",
            "epoch 800; error=0.003\n",
            "epoch 850; error=0.003\n",
            "epoch 900; error=0.003\n",
            "epoch 950; error=0.003\n",
            "epoch 1000; error=0.003\n",
            "epoch 1050; error=0.002\n",
            "epoch 1100; error=0.002\n",
            "epoch 1150; error=0.002\n",
            "epoch 1200; error=0.002\n",
            "epoch 1250; error=0.002\n",
            "epoch 1300; error=0.002\n",
            "epoch 1350; error=0.002\n",
            "epoch 1400; error=0.002\n",
            "epoch 1450; error=0.002\n",
            "epoch 1500; error=0.002\n",
            "epoch 1550; error=0.002\n",
            "epoch 1600; error=0.002\n",
            "epoch 1650; error=0.001\n",
            "epoch 1700; error=0.001\n",
            "epoch 1750; error=0.001\n",
            "epoch 1800; error=0.001\n",
            "epoch 1850; error=0.001\n",
            "epoch 1900; error=0.001\n",
            "epoch 1950; error=0.001\n",
            "Final result:\n",
            " Input: [0. 0.]\n",
            " Output: 0.026\n",
            " Target: 0.0\n",
            "\n",
            "Final result:\n",
            " Input: [1. 1.]\n",
            " Output: 1.0\n",
            " Target: 1.0\n",
            "\n",
            "Final result:\n",
            " Input: [1. 0.]\n",
            " Output: 0.984\n",
            " Target: 1.0\n",
            "\n",
            "Final result:\n",
            " Input: [0. 1.]\n",
            " Output: 0.984\n",
            " Target: 1.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNhyvJYdO1d4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kM96SSQlsJFD",
        "colab_type": "text"
      },
      "source": [
        "Now let's change the activation function to \"tanh\" from the \"logistic\" function, such that $g(\\textbf{net}) = \\tanh(\\textbf{net})$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgrVSQgkqTMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g_tanh(x):\n",
        "    return (torch.exp(x) - torch.exp(-x))/(torch.exp(x) + torch.exp(-x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ku7bguwwsa2d",
        "colab_type": "text"
      },
      "source": [
        "The derivative of the tanh function is as follows:\n",
        "\n",
        "$$\\frac{\\partial g(\\textbf{net})}{\\partial \\textbf{net}}= \\frac{\\partial \\tanh(\\textbf{net})}{\\partial \\textbf{net}} = 1.0 - (\\tanh(\\textbf{net}))^2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-XUPSLVqYeM",
        "colab_type": "code",
        "outputId": "c74cf0ea-aa2d-46da-e125-92d2741f8a68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Initialize parameters\n",
        "#     Although you will implement gradient descent manually, let's set requires_grad=True\n",
        "#     anyway so PyTorch will track the gradient too, and we can compare your gradient with PyTorch's.\n",
        "w = torch.rand(2,requires_grad=True) # [size 2] tensor\n",
        "b = torch.rand(1,requires_grad=True) # [size 1] tensor\n",
        "\n",
        "alpha = 0.05 # learning rate\n",
        "nepochs = 2000 # number of epochs\n",
        "\n",
        "track_error = []\n",
        "verbose = True\n",
        "for e in range(nepochs): # for each epoch\n",
        "    error_epoch = 0. # sum loss across the epoch\n",
        "    perm = np.random.permutation(N)\n",
        "    for p in perm: # visit data points in random order\n",
        "        x = D[p,:] # input pattern\n",
        "        \n",
        "        # compute output of neuron\n",
        "        net = torch.dot(x,w)+b\n",
        "        yhat = g_tanh(net)\n",
        "        \n",
        "        # compute loss\n",
        "        y = Y[p]\n",
        "        myloss = loss(yhat,y)\n",
        "        error_epoch += myloss.item()\n",
        "        \n",
        "        # print output if this is the last epoch\n",
        "        if (e == nepochs-1):\n",
        "            print(\"Final result:\")\n",
        "            print_forward(x,yhat,y)\n",
        "            print(\"\")\n",
        "\n",
        "        # Compute the gradient manually\n",
        "        if verbose:\n",
        "            print('Compute the gradient manually')\n",
        "            print_forward(x,yhat,y)\n",
        "        with torch.no_grad():\n",
        "            # TODO : YOUR GRADIENT CODE GOES HERE\n",
        "            #  two lines of the form\n",
        "            #    w_grad = ...    ([size 2] PyTorch tensor)\n",
        "            #    b_grad = ...    ([size 1] PyTorch tensor)\n",
        "            #  make sure to inclose your code in the \"with torch.no_grad()\" wrapper,\n",
        "            #   otherwise PyTorch will try to track the \"gradient\" of the graident computation, which we don't want.\n",
        "            w_grad = 2*(yhat-y)*(1-yhat**2)*x\n",
        "            b_grad = 2*(yhat-y)*(1-yhat**2)\n",
        "                                   \n",
        "        if verbose: print_grad(w_grad.numpy(),b_grad.numpy())\n",
        "\n",
        "        # Compute the gradient with PyTorch and compre with manual values\n",
        "        if verbose: print('Compute the gradient using PyTorch .backward()')\n",
        "        myloss.backward()\n",
        "        if verbose:\n",
        "            print_grad(w.grad.numpy(),b.grad.numpy())\n",
        "            print(\"\")\n",
        "        w.grad.zero_() # clear PyTorch's gradient\n",
        "        b.grad.zero_()\n",
        "        \n",
        "        # Parameter update with gradient descent\n",
        "        with torch.no_grad():\n",
        "            # TODO : YOUR PARAMETER UPDATE CODE GOES HERE\n",
        "            #  two lines of the form:\n",
        "            #    w -=   ....\n",
        "            #    b -=   ....\n",
        "            w -= w_grad\n",
        "            b -= b_grad\n",
        "            \n",
        "            \n",
        "    if verbose==True: verbose=False\n",
        "    track_error.append(error_epoch)\n",
        "    if e % 50 == 0:\n",
        "        print(\"epoch \" + str(e) + \"; error=\" +str(round(error_epoch,3)))\n",
        "    \n",
        "# track output of gradient descent\n",
        "plt.figure()\n",
        "plt.clf()\n",
        "plt.plot(track_error)\n",
        "plt.title('stochastic gradient descent (tanh activation)')\n",
        "plt.ylabel('error for epoch')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Compute the gradient manually\n",
            " Input: [0. 0.]\n",
            " Output: 0.628\n",
            " Target: 0.0\n",
            "  d_loss / d_w = [0. 0.]\n",
            "  d_loss / d_b = [0.7605431]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [0. 0.]\n",
            "  d_loss / d_b = [0.7605431]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [1. 0.]\n",
            " Output: 0.092\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-1.800984 -0.      ]\n",
            "  d_loss / d_b = [-1.800984]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [-1.8009839  0.       ]\n",
            "  d_loss / d_b = [-1.8009839]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [1. 1.]\n",
            " Output: 0.999\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-1.0849742e-06 -1.0849742e-06]\n",
            "  d_loss / d_b = [-1.0849742e-06]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [-1.0848744e-06 -1.0848744e-06]\n",
            "  d_loss / d_b = [-1.0848744e-06]\n",
            "\n",
            "Compute the gradient manually\n",
            " Input: [0. 1.]\n",
            " Output: 0.976\n",
            " Target: 1.0\n",
            "  d_loss / d_w = [-0.         -0.00222303]\n",
            "  d_loss / d_b = [-0.00222303]\n",
            "Compute the gradient using PyTorch .backward()\n",
            "  d_loss / d_w = [ 0.         -0.00222304]\n",
            "  d_loss / d_b = [-0.00222304]\n",
            "\n",
            "epoch 0; error=1.22\n",
            "epoch 50; error=0.003\n",
            "epoch 100; error=0.002\n",
            "epoch 150; error=0.001\n",
            "epoch 200; error=0.001\n",
            "epoch 250; error=0.001\n",
            "epoch 300; error=0.001\n",
            "epoch 350; error=0.001\n",
            "epoch 400; error=0.001\n",
            "epoch 450; error=0.0\n",
            "epoch 500; error=0.0\n",
            "epoch 550; error=0.0\n",
            "epoch 600; error=0.0\n",
            "epoch 650; error=0.0\n",
            "epoch 700; error=0.0\n",
            "epoch 750; error=0.0\n",
            "epoch 800; error=0.0\n",
            "epoch 850; error=0.0\n",
            "epoch 900; error=0.0\n",
            "epoch 950; error=0.0\n",
            "epoch 1000; error=0.0\n",
            "epoch 1050; error=0.0\n",
            "epoch 1100; error=0.0\n",
            "epoch 1150; error=0.0\n",
            "epoch 1200; error=0.0\n",
            "epoch 1250; error=0.0\n",
            "epoch 1300; error=0.0\n",
            "epoch 1350; error=0.0\n",
            "epoch 1400; error=0.0\n",
            "epoch 1450; error=0.0\n",
            "epoch 1500; error=0.0\n",
            "epoch 1550; error=0.0\n",
            "epoch 1600; error=0.0\n",
            "epoch 1650; error=0.0\n",
            "epoch 1700; error=0.0\n",
            "epoch 1750; error=0.0\n",
            "epoch 1800; error=0.0\n",
            "epoch 1850; error=0.0\n",
            "epoch 1900; error=0.0\n",
            "epoch 1950; error=0.0\n",
            "Final result:\n",
            " Input: [1. 1.]\n",
            " Output: 1.0\n",
            " Target: 1.0\n",
            "\n",
            "Final result:\n",
            " Input: [0. 0.]\n",
            " Output: -0.002\n",
            " Target: 0.0\n",
            "\n",
            "Final result:\n",
            " Input: [0. 1.]\n",
            " Output: 0.994\n",
            " Target: 1.0\n",
            "\n",
            "Final result:\n",
            " Input: [1. 0.]\n",
            " Output: 0.994\n",
            " Target: 1.0\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7QcVZn38e/vnCSAEK6JvhISQiAy\nMl4xIogXRkADo+AgCqgjIMr4Lhh1FBRGBhl0ZNRXdFyCyGgGVARERTMKA6Ko4yVAQEBAkHCTRCAh\nIBeBQJLn/WPvTqr7dPepczjVfQ71+6zVq7suveupS9fTtXddFBGYmVl9DfQ7ADMz6y8nAjOzmnMi\nMDOrOScCM7OacyIwM6s5JwIzs5pzIhinJIWkHXownYslHVr1dEZK0lmSPpk/v1rSLX2MpSfroiqS\n/kHSFyoqew9JS6sou8S0H5U0p4Jyx2R7k7SBpJslTR+LuKrkRDCGJJ0k6Zv9jqOTdvFFxD4RcXa/\nYiojIv43InYci7Ik3Slpr7Eoq98kHSbpl8OMMwU4Afhs7p6dE9ukXsQ4ViT9TNJ7iv0iYpOIuH0M\nym5K9GO1vUXEKmABcNzTLatqTgRWuYm203mG2R+4OSKW9TuQmvoWcKikDfodSFcR4dcIX8BHgWXA\nI8AtwJ7AfOBJ4CngUeC6PO7WwELgAWAJ8N5COYPAPwO35bKuBmbmYQG8D7gV+DNwGqA8bHvgp8BK\n4H7gHGDzUcb3M+A9he++F/h9/u5NwM4dlsHrc9kPAacDP2+UAxwG/Ar4fI7xkyVifilwTZ7u+cB5\nwCfzsD2ApYVxtwa+C6wA7gDeXxh2EvBt4Ou5rBuBeXnYN4C1wON5GXykw7wdC9wD/Al4d14XO+Rh\nGwD/D/gjcB9wBrBRHjYN+GFeXw8A/wsM5GEzge/lmFcCXypM7915mT8IXAJsWxjWdjsAng88AazJ\n8/LnDvOyADih0P3HXOaj+bVbiXVzJ3AMcH1e3+cDGxbXDfBhYHlebod3+e0czvrt63bgH1qG7w9c\nCzxM+l3MB/4tz+cTOeYvFZbNDsArgHuBwUI5fwdcnz/vAvwmL797gC8BU/KwX+Ry/pLLPoih29vz\nSb+TP5O2p/0Kw87K6+RHeZ6uALZvmadbgdf2e7/VdZ/W7wAm2gvYEbgb2Dp3z26seNJO6Jst4/+C\ntKPcEHgJaUfwujzsWOB3uUwBLwa2ysOCtFPZHJiVvzc/D9sB2Ju0U5qep/GFUcb3M9bvwN9KSiAv\nz/HsQGGnVPjOtPxDPQCYBHyAlGCKiWA18I95+EbDxDwFuAv4J2AycGAub0giIB3FXg2cmL83h7RD\neUNhHp8A9iUl2lOARYXY7wT26rJ+55N28C8ANib9oysmgs+TEvuWwFTgv4FT8rBTSIlhcn69Oi/H\nQeC6/N2N87bwqvyd/Ul/EJ6fl9UJwK8L8XTbDg4DfjnM9noV8NZC9+xc5qRCv47rprDMriQl4C1J\nO/L3FdbNauDkPM/7Ao8BW3SI529JiUfAa/O4O+dhu5ASzd55Pc8A/qp1O21ZNo31chuwd2HYBcBx\n+fPLgF3z8p2d4/9gu3LabG+T8/r5Z9L29jrSDn/HPPwsUgLdJZd/DnBeS5wLKfxZGY+vvgcw0V75\nR7Mc2AuY3DLsJAo7WtK/wDXA1EK/U4Cz8udbgP07TCfIO4vc/e3Ght1m3DcDvx1pfLnfuh8Y6d/o\nB0osg3cBvyl0i5R8iongj8OUUYz5NaR/3yoM/zXtE8ErWssGjgf+qzCPlxWG7QQ8Xui+k+6JYAHw\n74Xu57H+n6dI/xy3LwzfDbgjfz4Z+EFxp1IYZwWFnW9h2MXAEYXuAdLOcdvhtgPKJYJbyYkjd8+m\nJRF0WzeFZfbOQvdngDMK6+ZxmhPLcmDXkr+n7ze2OeArwOc7jLduO235jTQSwSeBBfnz1Lyetu1Q\n1geBC9uV02Z7ezXpaGOgMPxc4KT8+Szgq4Vh+5Kq4orTOwc4sczy6NfLbQQjFBFLSBvSScBySedJ\n2rrD6FsDD0TEI4V+d5H+6UBKFLd1mdy9hc+PAZsASHpOnu4ySQ8D3yT9Sx9pfK2Gi6dha9KOnzzN\nIFUPFN1d7OgWcy5vWS6n4a4O094W2FrSnxsv0r+15xTGaV1uG46gnaJp3lrimA48C7i6MO3/yf0h\nNcguAS6VdLukRiPhTOCuiFjdYX7+o1DeA6SEM6MwTtvtoKQHSTvGjoZZN2ViWNkybx1jlLSPpEWS\nHsjzu29hWmW3v3a+BRyQ6+IPAK6JiLvyNJ8n6YeS7s3z96k289fJ1sDdEbG20K/4G4bh189UUrXS\nuOVEMAoR8a2IeBXpRxzApxuDWkb9E7ClpOIPcRap+gXSDmf7UYTwqTytF0bEpsA7STuPkcbXqmw8\n9wDbNDokqdjdYVrdYr4HmJHLaZjVJcY7ImLzwmtqROxbIu52cbW6h7RDahfH/aR/v39dmPZmEbEJ\nQEQ8EhEfjog5wH7AhyTtmWOe1SEZ3U2qJy/Oz0YR8esxmBdI9frPG+Y7XbensZJ30t8ltbE8JyI2\nBy4qTKvb9td1XiPiJtIOeh/g7aTE0PBl4GZgbp6/f6b8/P0JmCmpuK8s/obLeD6panDcciIYIUk7\nSnpd3qifIO0YGv8W7gNmNzaaiLibVMVxiqQNJb0IOIL0jwvgq8AnJM1V8iJJW5UIYyqpYeshSTNI\nbQ0jjq+NrwLHSHpZjmcHSdu2Ge9HwAslvTnv3I4C/s9oYyY15K0G3i9psqQDSHWu7VwJPCLpo5I2\nkjQo6QWSXj7M9BvuI7UrdPJt4DBJO0l6FvDxxoD8r/A/gc9LejaApBmS3pA/vzEvM5HquteQlv2V\npATz75I2ztvC7rnYM4DjJf11LmMzSW8dwbxsk08R7eQiUl18w4ocU3EZdFs3Y2kKqR1iBbBa0j6k\nkw4avgYcLmlPSQN52f5VHjbceoO08/8AqarxgkL/qaQ2rUdzef+35Xvdyr6C9C//I3nb3AN4E+lk\nhmHl5bklsKjM+P3iRDByGwD/Tvp3eC/wbFIdNazf+FZKuiZ/PoRUL/sn4ELg4xFxWR52KmnHcylp\nQ/0aqWF1OP8K7Eza2fyIdDbKaONbJyIuIJ2h8S1Sg9j3SRtx63j3kxqWP0NqKNsJWAysGk3MEfEk\n6XD+MFLVyEEt81Sc9hrgjaSG9zvyfH4V2KzLtItOAU7IVTHHtCn/YuALpLNoluT3oo/m/otyNcNl\npAZ6gLm5+1FScjs9Ii7PMb+J1M7wR1I12kF5eheSjtjOy+XdQPpXW8ZPSWex3Cvp/g7j/DfwV43q\nwYh4jLSOf5WXwa50357GTK4ifT9pm3+Q9M99YWH4laSzij6fY/k56agW4D+AAyU9KOmLHSZxLinp\n/TRvow3H5Gk9Qkrk57d87yTg7Lw83tYS85OkdbcPaVs7HXhXRNxccrbfDpwd6ZqCcatxOqLZqOUj\njKXAOyLi8n7HY80kHQnsFBEf7HcsdZKPyq8DXhMRy/sdTzdOBDYquTrkClLV07Gk6qE5EfF4XwMz\nsxFz1ZCN1m6kMzzuJx06v9lJwGxi8hGBmVnN+YjAzKzmJtzNwKZNmxazZ8/udxhmZhPK1VdffX9E\ntL0l9oRLBLNnz2bx4sX9DsPMbEKR1OlqfVcNmZnVnROBmVnNORGYmdWcE4GZWc05EZiZ1ZwTgZlZ\nzTkRmJnVXG0SwR/ue4RTL72F+x8d13eDNTPrucoSgaQFkpZLuqHD8HdIul7S7yT9WtKLq4oF4Nb7\nHuWLP13CykefrHIyZmYTTpVHBGcB87sMvwN4bUS8EPgEcGaFsaAxf/CemdkzQ2W3mIiIX0ia3WV4\n8Zmsixj6zNtKRKnHvJqZ1cd4aSM4Ari400BJR0paLGnxihUrRjWBxgGB77ptZtas74lA0t+QEsFH\nO40TEWdGxLyImDd9etub55WYTqOsUX3dzOwZq693H5X0ItKDx/eJiJUVT63a4s3MJqi+HRFImgV8\nD/j7iPhDr6brNgIzs2aVHRFIOhfYA5gmaSnwcWAyQEScAZwIbAWcrlRvszoi5lUXT3p31ZCZWbMq\nzxo6ZJjh7wHeU9X0W7liyMysvb43FpuZWX/VJhHk6idXDZmZtahPIsjvbiw2M2tWn0TgRgIzs7Zq\nkwgaXDVkZtasNolg3emj/Q3DzGzcqU8ioNFY7FRgZlZUm0TgCwnMzNqrTyLIfDxgZtasNonAt6E2\nM2uvPonA54+ambVVm0Swng8JzMyKapMIXDVkZtZefRKBryMwM2urPonA54+ambVVm0TQ4KohM7Nm\ntUkE659Q5kxgZlZUn0SQ350GzMya1SYRmJlZe/VJBH54vZlZW7VJBOvuPurKITOzJvVJBD571Mys\nrdokgnV8QGBm1qQ2icBnDZmZtVdZIpC0QNJySTd0GC5JX5S0RNL1knauKpY8PcCNxWZmrao8IjgL\nmN9l+D7A3Pw6EvhyhbG4jcDMrIPKEkFE/AJ4oMso+wNfj2QRsLmk51YVz7q4XDlkZtakn20EM4C7\nC91Lc78hJB0pabGkxStWrBjVxHwbajOz9iZEY3FEnBkR8yJi3vTp00dVhm9DbWbWXj8TwTJgZqF7\nm9zPzMx6qJ+JYCHwrnz20K7AQxFxT3WTa5w15GMCM7OiSVUVLOlcYA9gmqSlwMeByQARcQZwEbAv\nsAR4DDi8qlhSPOndacDMrFlliSAiDhlmeABHVTX9Vj571MysvQnRWDymfEhgZtakNolg3ZXFzgRm\nZk3qkwjyu9uKzcya1ScRuJHAzKyt2iSCBh8RmJk1q00iWP+EMjMzK6pPIlj3zGKnAjOzotokAjMz\na692icDHA2ZmzWqTCNZXDfU3DjOz8aY+icA3mTAza6s2iWAgz+laHxKYmTWpTSKYlDPBU2vW9jkS\nM7PxpTaJYPJgqhpavcZHBGZmRbVJBJMG06yuXusjAjOzotokgskD6YjgKR8RmJk1qU8iGHQbgZlZ\nO7VJBJPcRmBm1lZtEsG6IwK3EZiZNalNIhjMbQRr1/qIwMysqD6JIN9jYrUTgZlZk9okggEfEZiZ\ntTVpuBEk7Q6cBGybxxcQETGn2tDG3uCAWONbTJiZNSlzRPA14FTgVcDLgXn5fViS5ku6RdISSce1\nGT5L0uWSfivpekn7jiT4kRqU8NmjZmbNhj0iAB6KiItHWrCkQeA0YG9gKXCVpIURcVNhtBOAb0fE\nlyXtBFwEzB7ptMoaGPBN58zMWnVMBJJ2zh8vl/RZ4HvAqsbwiLhmmLJ3AZZExO25vPOA/YFiIghg\n0/x5M+BPI4p+hCYNDLDGbQRmZk26HRF8rqV7XuFzAK8bpuwZwN2F7qXAK1rGOQm4VNI/AhsDe7Ur\nSNKRwJEAs2bNGmaynQ0IJwIzsxYdE0FE/E0Ppn8IcFZEfE7SbsA3JL0gIppq8iPiTOBMgHnz5o16\nTz44ICcCM7MWwzYWS/qUpM0L3VtI+mSJspcBMwvd2+R+RUcA3waIiN8AGwLTSpQ9Kj5ryMxsqDJn\nDe0TEX9udETEg0CZs3uuAuZK2k7SFOBgYGHLOH8E9gSQ9HxSIlhRJvDRGJB8HYGZWYsyiWBQ0gaN\nDkkbARt0GR+AiFgNHA1cAvyedHbQjZJOlrRfHu3DwHslXQecCxwWUd1f9kmuGjIzG6LM6aPnAD+R\n9F+5+3Dg7DKFR8RFpFNCi/1OLHy+Cdi9XKhP34ATgZnZEMMmgoj4dP7H3jij5xMRcUm1YVXDbQRm\nZkOVOSIA+C0wmXTa6G+rC6da6cpiJwIzs6IyZw29DbgSOBB4G3CFpAOrDqwKAwPylcVmZi3KHBF8\nDHh5RCwHkDQduAz4TpWBVcFHBGZmQ5U5a2igkQSylSW/N+6kC8r6HYWZ2fhS5ojgfyRdQjq9E+Ag\nWs4EmihSInAmMDMrKnPW0LGSDiDdhhrgzIi4sNqwqjEwIPzsejOzZmXPGvo1sAZYS7pieEIalJ9Q\nZmbWqsxZQ+8hnTX0d6QzhxZJenfVgVXBN50zMxuqzBHBscBLI2IlgKStSEcIC6oMrAoD8gVlZmat\nypz9sxJ4pND9SO434Uwa9BGBmVmrMkcES0gXkf2AdGXx/sD1kj4EEBGnVhjfmBqQLygzM2tVJhHc\nll8NP8jvU8c+nOo5D5iZNStz+ui/Akh6VkQ8Vn1I1ZGE84CZWbMyZw3tJukm4Obc/WJJp1ceWQXU\n7wDMzMahMo3FXwDeQG4gjojrgNdUGVSlXDdkZtak1D2DIuLull5rKoilchKuGjIza1GmsfhuSa8E\nQtJk4AOkR09OOMIHBGZmrcocEbwPOAqYASwDXpK7JxzJrQRmZq3KnDV0P/COHsTSE+HKITOzJhPy\nuQKj5aohM7Oh6pUI5ERgZtaqayKQNJCfWfwM4TYCM7NWXRNBRKwFPtKjWHrCBwRmZs3KVA1dJukY\nSTMlbdl4lSlc0nxJt0haIum4DuO8TdJNkm6U9K0RRT9CqWrIqcDMrKjMdQQH5ffiKaMBzOn2JUmD\nwGnA3sBS4CpJCyPipsI4c4Hjgd0j4kFJzx5J8CPliiEzs6HKnD663SjL3gVYEhG3A0g6j3QL65sK\n47wXOC0iHszTWj7KaZXiywjMzIYqc9O5yZLeL+k7+XV0vsJ4ODOA4q0pluZ+Rc8DnifpV5IWSZrf\nIYYjJS2WtHjFihUlJt2Za4bMzJqVaSP4MvAy4PT8elnuNxYmAXOBPYBDgP+UtHnrSBFxZkTMi4h5\n06dPH/XEhHxBmZlZizJtBC+PiBcXun8q6boS31sGzCx0b5P7FS0FroiIp4A7JP2BlBiuKlH+iPk6\nAjOzococEayRtH2jQ9Icyt199CpgrqTtJE0BDgYWtozzfdLRAJKmkaqKbi9R9qi4jcDMbKgyRwTH\nApdLup104s22wOHDfSkiVks6GrgEGAQWRMSNkk4GFkfEwjzs9fnBN2uAYyNi5SjnpRQfEJiZNeuY\nCCS9NSIuIP1DnwvsmAfdEhGryhQeERcBF7X0O7HwOYAP5VflhHwdgZlZi25VQ8fn9+9GxKqIuD6/\nSiWBcckPpjEzG6Jb1dBKSZcC20lqrdsnIvarLqxquInAzGyobongb4GdgW8An+tNOD3gQwIzsyYd\nE0FEPAkskvTKiHh6V3GNE5KcB8zMWgx7+ugzJQlA48E0TgVmZkW1ezCNmZk1G+7BNIOS/qlXwfSC\njwfMzJoN92CaNaR7AD0j+JnFZmZDlbmy+FeSvgScD/yl0TMirqksqoqkxmJnAjOzojKJ4CX5/eRC\nvwBeN/bhVMtNBGZmQ5V5MM3f9CKQXnHVkJlZszIPptlM0qmNB8NI+pykzXoR3JjzbajNzIYoc/ro\nAuAR4G359TDwX1UGVRW5csjMbIgybQTbR8RbCt3/KunaqgKqkq8jMDMbqswRweOSXtXokLQ78Hh1\nIVXLVxabmTUrc0TwPuDrhXaBB4FDqwupOsIXlJmZteqaCCQNADtGxIslbQoQEQ/3JLIK+JnFZmZD\nDXdl8VrgI/nzwxM5CYAbi83M2inTRnCZpGMkzZS0ZeNVeWQV8ZXFZmbNyrQRHJTfjyr0C2DO2IdT\nLVcNmZkNVaaN4J0R8asexVMp+ZnFZmZDlGkj+FKPYukBtxGYmbUq00bwE0lvkZ4Zl2O5asjMrFmZ\nRPAPwAXAk5IelvSIpFJnD0maL+kWSUskHddlvLdICknzSsY9KimVOROYmRWVufvo1NEULGkQOA3Y\nG1gKXCVpYUTc1DLeVOADwBWjmc6IYsJHBGZmrcrcfVSS3inpX3L3TEm7lCh7F2BJRNweEU8C5wH7\ntxnvE8CngSdGEPeouLHYzGyoMlVDpwO7AW/P3Y+S/ukPZwZwd6F7ae63jqSdgZkR8aNuBUk6snEb\n7BUrVpSYtJmZlVUmEbwiIo4i/2OPiAeBKU93wvnU1FOBDw83bkScGRHzImLe9OnTRz9N5JvOmZm1\nKJMInsr1/QEgaTqwtsT3lgEzC93b5H4NU4EXAD+TdCewK7CwygZjVw2ZmQ1VJhF8EbgQeLakfwN+\nCXyqxPeuAuZK2k7SFOBgYGFjYEQ8FBHTImJ2RMwGFgH7RcTikc5EWW4sNjMbqsxZQ+dIuhrYk7Qv\nfXNE/L7E91ZLOhq4BBgEFkTEjZJOBhZHxMLuJYy9Z8ilEGZmY6rMvYaIiJuBm0daeERcBFzU0u/E\nDuPuMdLyR8NtBGZmzcpUDT2jOA2YmTWrVSKQH1FmZjZEvRKBbzpnZjZErRIB+IDAzKxVrRJBejCN\nU4GZWVG9EgE+IjAza1WvROAmAjOzIWqVCMBXFpuZtapVIpBEuHLIzKxJvRIBPiIwM2tVq0TgywjM\nzIaqVyLAZw2ZmbWqVSIQfiCBmVmreiUC4cZiM7MW9UoE/Q7AzGwcqlUiAJ81ZGbWqlaJwM8sNjMb\nql6JAPmmc2ZmLeqVCNxIYGY2RK0SAbhqyMysVa0SgW8xYWY2VK0SgeuGzMyGqlUicBowMxuq0kQg\nab6kWyQtkXRcm+EfknSTpOsl/UTStlXG0+Azh8zM1qssEUgaBE4D9gF2Ag6RtFPLaL8F5kXEi4Dv\nAJ+pKp4UU3p3HjAzW6/KI4JdgCURcXtEPAmcB+xfHCEiLo+Ix3LnImCbCuNJN53DZw6ZmRVVmQhm\nAHcXupfmfp0cAVxcYTxuKzYza2NSvwMAkPROYB7w2g7DjwSOBJg1a9bTnl5qI3BWMDODao8IlgEz\nC93b5H5NJO0FfAzYLyJWtSsoIs6MiHkRMW/69OmjDqix63fVkJnZelUmgquAuZK2kzQFOBhYWBxB\n0kuBr5CSwPIKY8nTS+9uLDYzW6+yRBARq4GjgUuA3wPfjogbJZ0sab882meBTYALJF0raWGH4saE\n3EhgZjZEpW0EEXERcFFLvxMLn/eqcvqd+CllZmbr1erK4gZXDZmZrVerROCaITOzoeqVCHzKqJnZ\nELVKBA2uGjIzW69WiWDd6aNuLDYzW6deiSC/+4jAzGy9eiUCNxGYmQ1Rq0TQ4AMCM7P1apUI1t2G\n2nVDZmbr1CsRrGssNjOzhlolAjMzG6qWicA1Q2Zm69UqEch1Q2ZmQ9QrEeR3X1BmZrZevRKBryMw\nMxuiVomgwW0EZmbr1SoR+JnFZmZD1SoRPLpqNQDLHny8z5GYmY0ftUoEO229KQCPrHqqz5GYmY0f\ntUoEm200GYCn1rhyyMysoVaJYPJgmt2nVq/tcyRmZuNHLRPB6rVOBGZmDbVMBE+6asjMbJ2aJYJ0\nAuld9/+lz5GYmY0flSYCSfMl3SJpiaTj2gzfQNL5efgVkmZXGc+WG08B4HM//gMLfnkHS5Y/UuXk\nzMwmhElVFSxpEDgN2BtYClwlaWFE3FQY7QjgwYjYQdLBwKeBg6qKaeqGk3nl9lvx69tWcvIPUxhz\npm/MUXvswKYbTWZwIJ1ZtMGkQTacPMAGkwaZMmmAhx5/ig0nDQKw0ZRBtnjWZAYkpMKN7MzMJihV\n9bQuSbsBJ0XEG3L38QARcUphnEvyOL+RNAm4F5geXYKaN29eLF68eNRxrVkbnPD9Gzj3yj+OuoxW\nEgxIDCg9BS0liEa/9Fy0RtIYyP2754/OA7t9r1uR3b9XxfRGlyC7Tm+M52G08Tv1j8AEWVgTJEwO\n2WUW73n1nFF9V9LVETGv3bDKjgiAGcDdhe6lwCs6jRMRqyU9BGwF3F8cSdKRwJEAs2bNelpBDQ6I\nUw54Iacc8EKeeGoN9z70BI89uYZHV61m8qB4+InVrHpqDU+sXsuqp9awavVaJg+KSQMDPLlmLY/n\ncSNgbaT7mEbEuu61ke5uGpH6r22Ml7uDlIw66ZaWu+fsLmV2+V7XYaMts/OgUU9vlIM6PpZ09DFa\nWRPlkbATI8pk2iYbVFJulYlgzETEmcCZkI4IxqrcDScPMnvaxmNVnJnZhFRlY/EyYGahe5vcr+04\nuWpoM2BlhTGZmVmLKhPBVcBcSdtJmgIcDCxsGWchcGj+fCDw027tA2ZmNvYqqxrKdf5HA5cAg8CC\niLhR0snA4ohYCHwN+IakJcADpGRhZmY9VGkbQURcBFzU0u/EwucngLdWGYOZmXVXqyuLzcxsKCcC\nM7OacyIwM6s5JwIzs5qr7BYTVZG0ArhrlF+fRstVy+PEeI0Lxm9sjmtkHNfIPBPj2jYiprcbMOES\nwdMhaXGne23003iNC8ZvbI5rZBzXyNQtLlcNmZnVnBOBmVnN1S0RnNnvADoYr3HB+I3NcY2M4xqZ\nWsVVqzYCMzMbqm5HBGZm1sKJwMys5mqTCCTNl3SLpCWSjuvxtGdKulzSTZJulPSB3P8kScskXZtf\n+xa+c3yO9RZJb6gwtjsl/S5Pf3Hut6WkH0u6Nb9vkftL0hdzXNdL2rmimHYsLJNrJT0s6YP9WF6S\nFkhaLumGQr8RLx9Jh+bxb5V0aLtpjUFcn5V0c572hZI2z/1nS3q8sNzOKHznZXn9L8mxP62nNnaI\na8Trbax/rx3iOr8Q052Srs39e7m8Ou0beruNpccsPrNfpNtg3wbMAaYA1wE79XD6zwV2zp+nAn8A\ndgJOAo5pM/5OOcYNgO1y7IMVxXYnMK2l32eA4/Ln44BP58/7AheTHvG6K3BFj9bdvcC2/VhewGuA\nnYEbRrt8gC2B2/P7FvnzFhXE9XpgUv786UJcs4vjtZRzZY5VOfZ9KohrROutit9ru7hahn8OOLEP\ny6vTvqGn21hdjgh2AZZExO0R8SRwHrB/ryYeEfdExDX58yPA70nPa+5kf+C8iFgVEXcAS0jz0Cv7\nA2fnz2cDby70/3oki4DNJT234lj2BG6LiG5Xk1e2vCLiF6RnZbRObyTL5w3AjyPigYh4EPgxMH+s\n44qISyNide5cRHoqYEc5tk0jYlGkvcnXC/MyZnF10Wm9jfnvtVtc+V/924Bzu5VR0fLqtG/o6TZW\nl0QwA7i70L2U7jviykiaDbwUuCL3Ojof4i1oHP7R23gDuFTS1ZKOzP2eExH35M/3As/pQ1wNB9P8\nA+338oKRL59+LLd3k/45Nmwn6beSfi7p1bnfjBxLL+IayXrr9fJ6NXBfRNxa6Nfz5dWyb+jpNlaX\nRDAuSNoE+C7wwYh4GPgysGZQ23wAAAP2SURBVD3wEuAe0uFpr70qInYG9gGOkvSa4sD8z6cv5xgr\nPeJ0P+CC3Gs8LK8m/Vw+nUj6GLAaOCf3ugeYFREvBT4EfEvSpj0MadyttxaH0Pxno+fLq82+YZ1e\nbGN1SQTLgJmF7m1yv56RNJm0os+JiO8BRMR9EbEmItYC/8n66oyexRsRy/L7cuDCHMN9jSqf/L68\n13Fl+wDXRMR9Oca+L69spMunZ/FJOgx4I/COvAMhV72szJ+vJtW/Py/HUKw+qiSuUay3Xi6vScAB\nwPmFeHu6vNrtG+jxNlaXRHAVMFfSdvlf5sHAwl5NPNdBfg34fUScWuhfrF//O6BxRsNC4GBJG0ja\nDphLaqQa67g2ljS18ZnU2HhDnn7jrINDgR8U4npXPnNhV+ChwuFrFZr+qfV7eRWMdPlcArxe0ha5\nWuT1ud+YkjQf+AiwX0Q8Vug/XdJg/jyHtHxuz7E9LGnXvI2+qzAvYxnXSNdbL3+vewE3R8S6Kp9e\nLq9O+wZ6vY09nRbvifQitbb/gZTdP9bjab+KdGh3PXBtfu0LfAP4Xe6/EHhu4Tsfy7HewtM8M6FL\nXHNIZ2RcB9zYWC7AVsBPgFuBy4Atc38Bp+W4fgfMq3CZbQysBDYr9Ov58iIlonuAp0j1rkeMZvmQ\n6uyX5NfhFcW1hFRP3NjGzsjjviWv32uBa4A3FcqZR9ox3wZ8iXy3gTGOa8Trbax/r+3iyv3PAt7X\nMm4vl1enfUNPtzHfYsLMrObqUjVkZmYdOBGYmdWcE4GZWc05EZiZ1ZwTgZlZzTkRmPWQpD0k/bDf\ncZgVORGYmdWcE4FZG5LeKelKpfvRf0XSoKRHJX1e6b7xP5E0PY/7EkmLtP45AI17x+8g6TJJ10m6\nRtL2ufhNJH1H6dkB5+SrS836xonArIWk5wMHAbtHxEuANcA7SFc7L46IvwZ+Dnw8f+XrwEcj4kWk\nqz0b/c8BTouIFwOvJF3ZCukOkx8k3Xd+DrB75TNl1sWkfgdgNg7tCbwMuCr/Wd+IdNOvtay/Odk3\nge9J2gzYPCJ+nvufDVyQ7+E0IyIuBIiIJwByeVdGvreN0lOxZgO/rH62zNpzIjAbSsDZEXF8U0/p\nX1rGG+39WVYVPq/Bv0PrM1cNmQ31E+BASc+Gdc+P3Zb0ezkwj/N24JcR8RDwYOHhJX8P/DzS06aW\nSnpzLmMDSc/q6VyYleR/ImYtIuImSSeQntw2QLpj5VHAX4Bd8rDlpHYESLcJPiPv6G8HDs/9/x74\niqSTcxlv7eFsmJXmu4+alSTp0YjYpN9xmI01Vw2ZmdWcjwjMzGrORwRmZjXnRGBmVnNOBGZmNedE\nYGZWc04EZmY19/8BfBFqrU4SaioAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}